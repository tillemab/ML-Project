{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ac4afe-eb2c-485b-92d1-503d99c72467",
   "metadata": {},
   "source": [
    "# YouTube Spam Comment Classifier\n",
    "---\n",
    "\n",
    "## Project Information\n",
    "- **Author:** Braden Tillema\n",
    "- **Date:** December 9, 2025\n",
    "- **School:** University of Colorado, Denver\n",
    "- **Course:** CSCI 5930-H01 (Machine Learning)\n",
    "\n",
    "## Project Objective\n",
    "The objective of this project was to develop a Google Chrome browser extension prototype that detects spam comments using Machine Learning. If a comment or reply is classified as spam, it is highlighted red. I created a binary spam comment classifier using Naive Bayes and Logistic Regression using Stochastic Gradient Descent. Using Ensemble Learning, both classifiers vote to determine if a comment is spam. The votes are weighted by the accuracy of the model divided by the sum of the accuracy of each model. This method enables the most accurate model to have the more important vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac21832-3f21-49ba-906e-444a2ca9a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv, path\n",
    "from requests import get\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790901d-ae1f-4b17-ac52-27f312bbc8e0",
   "metadata": {},
   "source": [
    "## Task 1. Opening the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df1b78-710e-4dee-ba54-ca4f8a05a0a7",
   "metadata": {},
   "source": [
    "### Merging Multiple Datasets\n",
    "This cell combines multiple datasets. Do **not** run this if you do not wish to reset any manual classifications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f5f88-cc3f-43e9-af71-ea0805615f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this was already created\n",
    "if path.exists('Datasets/Youtube-Spam-Collection.csv'):\n",
    "    input(\"Warning: Continuing resets any manual classifications! Press enter to proceed...\")\n",
    "\n",
    "# Import Datasets\n",
    "dataset_1 = pd.read_csv('Datasets/Youtube01-Psy.csv')\n",
    "dataset_2 = pd.read_csv('Datasets/Youtube02-KatyPerry.csv')\n",
    "dataset_3 = pd.read_csv('Datasets/Youtube03-LMFAO.csv')\n",
    "dataset_4 = pd.read_csv('Datasets/Youtube04-Eminem.csv')\n",
    "dataset_5 = pd.read_csv('Datasets/Youtube05-Shakira.csv')\n",
    "\n",
    "# Combine Datasets\n",
    "dataset = pd.concat([dataset_1, dataset_2, dataset_3, dataset_4, dataset_5], ignore_index=True)\n",
    "\n",
    "# Check that there are the expected number of samples\n",
    "assert len(dataset) == 1956, \"Incorrect Number of Samples!\"\n",
    "\n",
    "# Save Combined Dataset to CSV\n",
    "dataset.to_csv('Datasets/Youtube-Spam-Collection.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a9c0b-5720-4675-8b3a-22971ae6410a",
   "metadata": {},
   "source": [
    "### Opening an Existing Dataset\n",
    "Run this if `Youtube-Spam-Collection.csv` already exists and you already have manual classifications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060b0934-2a47-409b-9628-dd1920f102e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Combined Dataset\n",
    "dataset = pd.read_csv('Datasets/Youtube-Spam-Collection.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f93edf1-5939-40a8-9f52-4ade2f98872c",
   "metadata": {},
   "source": [
    "### Optional. Manually Classify Newer Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a44a3c9-63aa-43d9-96f6-fd38166e5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve YouTube API Key\n",
    "load_dotenv()\n",
    "YOUTUBE_API_KEY = getenv(\"YOUTUBE_API_KEY\")\n",
    "assert YOUTUBE_API_KEY != None, \"Missing API Key!\"\n",
    "\n",
    "# Format Request URL\n",
    "base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "part = \"id,replies,snippet\"\n",
    "videoId = \"1voqPZSqSE4\"\n",
    "\n",
    "# Construct Request URL\n",
    "req_url = f'{base_url}?key={YOUTUBE_API_KEY}&part={part}&videoId={videoId}'\n",
    "\n",
    "# Retrieve Comment Thread from YouTube API\n",
    "response = get(req_url)\n",
    "assert response.ok == True, f'Request returned {response.status_code}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142427ab-5ecc-4356-b723-83e2f2d98796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Response to JSON\n",
    "response_json = response.json()\n",
    "\n",
    "# Get the Next Page Token\n",
    "nextPageToken = response_json.get('nextPageToken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b0ef8-5460-4b6f-a581-e65b87ccbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all of the comments\n",
    "comments_data = response_json.get('items')\n",
    "\n",
    "print('Number of Top Level Comments: 0')\n",
    "while len(comments_data) < 30:\n",
    "\n",
    "    # Wait 5 seconds before requesting again\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Construct next Request URL using the last page token and get response\n",
    "    response = get(f'{req_url}&pageToken={nextPageToken}')\n",
    "\n",
    "    # If the response is not okay, exit loop early\n",
    "    if response.ok == False: break\n",
    "\n",
    "    # Convert Response to JSON\n",
    "    response_json = response.json()\n",
    "    \n",
    "    # Get the Next Page Token\n",
    "    nextPageToken = response_json.get('nextPageToken')\n",
    "\n",
    "    comments_data.append(response_json.get('items'))\n",
    "    print('Number of Top Level Comments:', len(comments_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9d630-558f-4f09-b72b-7eddc05c1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "\n",
    "comment_text = []\n",
    "\n",
    "for comment_data in comments_data:\n",
    "    if(type(comment_data) is dict): comments.append(comment_data)\n",
    "    else: comments.extend(comment_data)\n",
    "\n",
    "getText = lambda comment: comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "\n",
    "for comment in comments:\n",
    "    comment_text.append(getText(comment))\n",
    "    if 'replies' in comment.keys():\n",
    "        for reply in comment.get('replies').values():\n",
    "            comment_text.append(reply[0].get('textDisplay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84f853-852d-4e8c-b1d1-ea75f4bb5eb6",
   "metadata": {},
   "source": [
    "### Manual Classifying\n",
    "\n",
    "#### Inputs\n",
    "- `0` Marks the comment as Non-Spam\n",
    "- `1` Marks the comment as Spam\n",
    "- `2` End manual classifying\n",
    "- `3` Skip the comment\n",
    "\n",
    "#### Personal Criteria\n",
    "- If the comment is not in English, skip the comment\n",
    "- If the comment can not be understood, skip the comment\n",
    "- If the comment is advertising or asking for money, mark it as spam\n",
    "- If the comment is related to the video, mark it as non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ca3e6-2f19-41fb-ae23-fd85d75f215b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for comment in comment_text:\n",
    "\n",
    "    print(comment)\n",
    "    is_spam = int(input(\"Non-Spam (0), Spam (1), Skip (2), End (3): \"))\n",
    "\n",
    "    if is_spam == 3:\n",
    "        break\n",
    "\n",
    "    if is_spam == 2:\n",
    "        continue\n",
    "\n",
    "    if is_spam == 0 or is_spam == 1:\n",
    "        dataset.loc[len(dataset)] = [None, None, None, comment, is_spam]\n",
    "\n",
    "# Save Updated Dataset to CSV\n",
    "dataset.to_csv('Datasets/Youtube-Spam-Collection.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad83ef-3814-40c9-8f3b-23c5d2533ede",
   "metadata": {},
   "source": [
    "## Task 2. Pre-Processing\n",
    "### Determining the Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c561fe-7322-4158-bbec-f21cd3934221",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Compute the number of training and test samples\n",
    "training_samples = int(num_samples * 0.8)\n",
    "test_samples = num_samples - training_samples\n",
    "\n",
    "training_indices = indices[:training_samples]\n",
    "test_indices = indices[training_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6a0ac-4a57-48b8-b9d3-b2242deaef03",
   "metadata": {},
   "source": [
    "### Determine the Frequency of Each Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66a093f-e02a-4055-9f31-62dce00adf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 5606\n"
     ]
    }
   ],
   "source": [
    "# Frequency of each word, default to 0\n",
    "word_frequency = defaultdict(int)\n",
    "\n",
    "# Iterate through every comment in the training set\n",
    "for row in dataset.iloc[training_indices].itertuples(index = False):\n",
    "\n",
    "    # Retrieve the comment in lowercase\n",
    "    comment = row.CONTENT.lower()\n",
    "\n",
    "    # Split the comment by words and punctuation\n",
    "    comment_tk = comment.split(' ')\n",
    "\n",
    "    # For each token, increment frequency count\n",
    "    for word in comment_tk:\n",
    "        word_frequency[word] += 1\n",
    "\n",
    "print('Total Words:', len(word_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966a330-e4f0-481a-8d6a-12108511141e",
   "metadata": {},
   "source": [
    "### Remove Unfrequent Words\n",
    "I assume any unfrequent words are not global indication of spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edd24d10-a3ea-472f-80fa-4ded522e2b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Words: 640\n"
     ]
    }
   ],
   "source": [
    "# Create a list of words who appear more than 4 times\n",
    "words = [word for word, freq in word_frequency.items() if freq > 4]\n",
    "\n",
    "print('Remaining Words:', len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4852b5-43a9-445b-9f2d-31d651d18810",
   "metadata": {},
   "source": [
    "### Reconstruct Dataset Using Word Count as Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a2c4a72-80bd-4785-b70b-881454c5b097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>just</th>\n",
       "      <th>dance</th>\n",
       "      <th>3</th>\n",
       "      <th>hello</th>\n",
       "      <th>everyone</th>\n",
       "      <th>:)</th>\n",
       "      <th>i</th>\n",
       "      <th>know</th>\n",
       "      <th>most</th>\n",
       "      <th>of</th>\n",
       "      <th>...</th>\n",
       "      <th>8</th>\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>loved</th>\n",
       "      <th>~</th>\n",
       "      <th>ur</th>\n",
       "      <th>phenomenallyricshere</th>\n",
       "      <th>dress</th>\n",
       "      <th>tube</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 641 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   just  dance  3  hello  everyone  :)  i  know  most  of  ...  8     6  \\\n",
       "0     0      0  0      0         0   0  0     0     0   0  ...  0  0  0   \n",
       "1     0      0  0      0         0   0  1     0     0   0  ...  0  0  0   \n",
       "2     1      0  0      0         0   0  1     0     0   0  ...  0  0  0   \n",
       "3     0      0  0      0         0   0  0     0     0   0  ...  0  0  0   \n",
       "4     0      0  0      0         0   0  0     0     0   0  ...  0  0  0   \n",
       "\n",
       "   loved  ~  ur  phenomenallyricshere  dress  tube  Spam  \n",
       "0      0  0   0                     0      0     1     1  \n",
       "1      0  0   0                     0      0     0     1  \n",
       "2      0  0   0                     0      0     0     1  \n",
       "3      0  0   0                     0      0     0     1  \n",
       "4      0  0   0                     0      0     0     1  \n",
       "\n",
       "[5 rows x 641 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to construct the DataFrame\n",
    "comment_dataset = {word: [] for word in words}\n",
    "comment_dataset['Spam'] = []\n",
    "\n",
    "# Iterate through every comment\n",
    "for row in dataset.itertuples(index = False):\n",
    "\n",
    "    # Initialize the next index of the dictionary lists\n",
    "    for col in comment_dataset:\n",
    "        comment_dataset[col].append(0)\n",
    "\n",
    "    # Set the Class Label\n",
    "    comment_dataset['Spam'][-1] = row.CLASS\n",
    "\n",
    "    # Retrieve the comment in lowercase\n",
    "    comment = row.CONTENT.lower()\n",
    "\n",
    "    # Split the comment by words and punctuation\n",
    "    comment_tk = word_tokenize(comment)\n",
    "\n",
    "    # Increment the count in the dataset\n",
    "    for word in comment_tk:\n",
    "        if word in comment_dataset.keys():\n",
    "            comment_dataset[word][-1] += 1\n",
    "\n",
    "# Create the DataFrame\n",
    "comment_df = pd.DataFrame(comment_dataset)\n",
    "comment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72d32e-bd64-463b-b780-dc8a63e5d7e3",
   "metadata": {},
   "source": [
    "### Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968d7bd0-5f47-4242-b1f3-5c854f53a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_np = comment_df.iloc[training_indices].to_numpy()\n",
    "\n",
    "# Compute the correlation coefficient of each word\n",
    "corr_coeff = {}\n",
    "\n",
    "n = comment_np.shape[0]\n",
    "y = comment_np[:, -1]\n",
    "y_2 = y ** 2\n",
    "for i in range(len(words)):\n",
    "    x = comment_np[:, i]\n",
    "    xy = x * y\n",
    "    x_2 = x ** 2\n",
    "\n",
    "    numer = n * np.sum(xy) - np.sum(x) * np.sum(y)\n",
    "    denom = (n * np.sum(x_2) - np.sum(x) ** 2) * (n * np.sum(x_2) - np.sum(x) ** 2)\n",
    "\n",
    "    if denom != 0:\n",
    "        corr_coeff[words[i]] = abs(numer / np.sqrt(denom))\n",
    "    else:\n",
    "        corr_coeff[words[i]] = 0\n",
    "\n",
    "# Sort words by correlation coefficient strength\n",
    "words.sort(key = lambda x: corr_coeff[x], reverse = True)\n",
    "\n",
    "# Onlu keep the top 500 words\n",
    "for word in words[500:]:\n",
    "    comment_df.drop(word, axis = 1, inplace = True)\n",
    "words = words[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6f43e-f93e-433c-a04d-b902f530c213",
   "metadata": {},
   "source": [
    "### Create the Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30aa3621-58b6-4773-9f34-f855d91bd6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sets\n",
    "train_set = comment_df.iloc[training_indices].to_numpy()\n",
    "test_set = comment_df.iloc[test_indices].to_numpy()\n",
    "\n",
    "X_train = train_set[:, :-1]\n",
    "y_train = train_set[:, -1]\n",
    "\n",
    "X_test = test_set[:, :-1]\n",
    "y_test = test_set[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae20ce-0e17-418e-a65f-a03f797a7846",
   "metadata": {},
   "source": [
    "## Task 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963bf103-84ea-4a24-95da-9e07d6b7a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_logistic_regression(X_train, y_train, alpha, epochs):\n",
    "    \"Stochastic Gradient Descent for Logistic Regression\"\n",
    "\n",
    "    # Sigmoid Function used for Logistic Regression\n",
    "    h_beta = lambda X, betas: 1 / (1 + np.exp(-1 * X @ betas))\n",
    "\n",
    "    n = X_train.shape[0] # Number of Samples\n",
    "    m = X_train.shape[1] # Number of Features\n",
    "    \n",
    "    # Initialize Betas (Including Bias)\n",
    "    betas = np.random.uniform(size = (m + 1))\n",
    "\n",
    "    # Create Data Array with Bias Column\n",
    "    data = np.column_stack((np.ones(n), X_train, y_train))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        # Shuffle the Data Array\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        for i in range(n):\n",
    "            X_i = data[i, :-1]\n",
    "            y_i = data[i, -1]\n",
    "\n",
    "            gradient = X_i * (h_beta(X_i, betas) - y_i)\n",
    "            betas = betas - alpha * gradient\n",
    "\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3bb1f65-4d86-4a7d-9903-e2e22dda0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_predict(X_test, betas):\n",
    "\n",
    "    # Include the bias term column\n",
    "    x0 = np.ones(X_test.shape[0])\n",
    "    X = np.column_stack((x0, X_test))\n",
    "\n",
    "    # Return the sigmoid function result\n",
    "    return 1 / (1 + np.exp(-1 * (X @ betas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d1c63dc-fb5b-465c-8593-8c7d4d7b0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test, y_predict):\n",
    "    return np.mean(y_test == y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da7be3-171c-4a8e-a81e-1a1a4c288882",
   "metadata": {},
   "source": [
    "## Task 4. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc023362-ce20-4146-a094-21e8770f5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_training(X_train, y_train):\n",
    "\n",
    "    # Get the different target labels\n",
    "    targets = np.unique(y_train)\n",
    "\n",
    "    # Create a dictionary to store the mean and variance for each target label\n",
    "    statistics = {target: {\"mean\": 0, \"var\": 0} for target in targets}\n",
    "\n",
    "    for target in targets:\n",
    "\n",
    "        # Get the indices with the current target label\n",
    "        indices = np.where(y_train == target)\n",
    "\n",
    "        # Get the data for only those indices\n",
    "        data = X_train[indices]\n",
    "\n",
    "        # Compute the statistics for the current target label\n",
    "        statistics[target][\"mean\"] = np.mean(data, axis = 0)\n",
    "        statistics[target][\"var\"] = np.var(data, axis = 0) + 1e-9 # Prevent 0s\n",
    "\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e312faa3-b1e4-4345-af02-dfbfbd023c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(X, mean, var):\n",
    "    \"\"\"Normal Distribution Probability of a Sample\"\"\"\n",
    "    exponent = -1 * (X - mean) ** 2 / (2 * var)\n",
    "    denominator = np.sqrt(2 * np.pi * var)\n",
    "    return np.exp(exponent) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0c2430-74d1-4680-aa2c-93f911defac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(X_test, model):\n",
    "\n",
    "    y_predict = []\n",
    "\n",
    "    # Iterate through every sample\n",
    "    for sample in X_test:\n",
    "\n",
    "        highest_target = 0\n",
    "        highest_prob = float('-inf')\n",
    "\n",
    "        # Test for all target labels\n",
    "        for target in model:\n",
    "            mean = model[target][\"mean\"]\n",
    "            var = model[target][\"var\"]\n",
    "            prob = np.prod(probability(sample, mean, var))\n",
    "\n",
    "            if prob > highest_prob:\n",
    "                highest_target = target\n",
    "                highest_prob = prob\n",
    "\n",
    "        y_predict.append(highest_target)\n",
    "\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f02cf-c687-4349-a7e2-8f57ce3cfcb4",
   "metadata": {},
   "source": [
    "## Task 5. Ensemble Learning\n",
    "Combine the Naive Bayes and Logistic Regression Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b8595af-5d03-4d15-85c7-29ae438e3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_learning(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Obtain Naive Bayes and Logistic Regression Models based on Training and Test Sets\"\"\"\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Model 1. Naive Bayes\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Obtain the mean and variance for each feature for each target class\n",
    "    naive_bayes_model = naive_bayes_training(X_train, y_train)\n",
    "\n",
    "    # Get the predictions for the test set\n",
    "    y_predict = naive_bayes_predict(X_test, naive_bayes_model)\n",
    "\n",
    "    # Compute the accuracy of the model\n",
    "    naive_bayes_accuracy = accuracy(y_test, y_predict)\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Model 2. Stochastic Gradient Descent Logistic Regression\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Values for trial-and-error hyper-parameter search\n",
    "    alphas = [0.001, 0.005, 0.01]\n",
    "    epochs = [50, 100, 250, 500]\n",
    "\n",
    "    # Store the weights that yielded the highest model accuracy\n",
    "    logistic_regression_model = None\n",
    "    logistic_regression_accuracy = float('-inf')\n",
    "\n",
    "    # Test every combination of alphas and epochs\n",
    "    for alpha in alphas:\n",
    "        for epoch in epochs:\n",
    "\n",
    "            # Use Stochastic Gradient Descent to obtain the betas\n",
    "            betas = stochastic_gradient_descent_logistic_regression(X_train, y_train, alpha, epoch)\n",
    "\n",
    "            # Get the predictions for the test set\n",
    "            y_predict = np.round(logistic_regression_predict(X_test, betas))\n",
    "\n",
    "            # Compute the accuracy of the current model\n",
    "            curr_accuracy = accuracy(y_test, y_predict)\n",
    "\n",
    "            # If the result performed better, save it\n",
    "            if curr_accuracy > logistic_regression_accuracy:\n",
    "                logistic_regression_model = betas\n",
    "                logistic_regression_accuracy = curr_accuracy\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Calculate Model Weights Based on Accuracy\n",
    "    # ==========================================================================\n",
    "    total_accuracy = naive_bayes_accuracy + logistic_regression_accuracy\n",
    "    naive_bayes_weight = naive_bayes_accuracy / total_accuracy\n",
    "    logistic_regression_weight = logistic_regression_accuracy / total_accuracy\n",
    "\n",
    "    return naive_bayes_model, naive_bayes_weight, logistic_regression_model, logistic_regression_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d55c9760-968b-4bac-88e9-91206cb4f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(X_test, naive_bayes_model, naive_bayes_weight, logistic_regression_model, logistic_regression_weight):\n",
    "    \"\"\"Predict using the Naive Bayes and Logistic Regression Models\"\"\"\n",
    "\n",
    "    # Get predictions from the naive bayes model\n",
    "    y_predict_nb = naive_bayes_predict(X_test, naive_bayes_model)\n",
    "\n",
    "    # Get predictions from the logistic regression model\n",
    "    y_predict_lr = logistic_regression_predict(X_test, logistic_regression_model)\n",
    "\n",
    "    # Weight predictions and return the final prediction (0: Non-Spam, 1: Spam)\n",
    "    return np.round(naive_bayes_weight * y_predict_nb + logistic_regression_weight * y_predict_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc524985-91e6-4b1b-b8aa-6c48f9c46dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\03sno\\anaconda3\\envs\\gymnasium-env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\03sno\\anaconda3\\envs\\gymnasium-env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Weight: 0.43686502177068215\n",
      "Logisitc Regression Weight: 0.5631349782293179\n"
     ]
    }
   ],
   "source": [
    "# Obtain the Naive Bayes and Logistic Regression Models\n",
    "nb_model, nb_weight, lr_model, lr_weight = ensemble_learning(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Print the vote weights of each model\n",
    "print('Naive Bayes Weight:', nb_weight)\n",
    "print('Logisitc Regression Weight:', lr_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f0823f1-4f58-41a9-86a1-c9eca057159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8843373493975903\n"
     ]
    }
   ],
   "source": [
    "# Use the models to predict the test set\n",
    "y_predict = ensemble_predict(X_test, nb_model, nb_weight, lr_model, lr_weight)\n",
    "\n",
    "# Print out the accuracy on the test set\n",
    "print(accuracy(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8d429-8a83-468b-8fdb-09465ce30427",
   "metadata": {},
   "source": [
    "## Task 6. Save the Models\n",
    "Save the models to `models.json` to be used for the browser extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae6c56ed-ff99-4706-9ff1-07538e67f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to file models.json!\n"
     ]
    }
   ],
   "source": [
    "# Fix Numpy Objects to Store in JSON\n",
    "lr_model_fixed = [float(val) for val in lr_model]\n",
    "nb_model_fixed = {}\n",
    "for target in nb_model:\n",
    "    nb_model_fixed[int(target)] = {'mean': None, 'var': None}\n",
    "    nb_model_fixed[target]['mean'] = [float(val) for val in nb_model[target]['mean']]\n",
    "    nb_model_fixed[target]['var'] = [float(val) for val in nb_model[target]['var']]\n",
    "\n",
    "# Construct JSON Object to Save\n",
    "save_obj = {\n",
    "    'words': words,\n",
    "    'lr_model': lr_model_fixed,\n",
    "    'lr_weight': float(lr_weight),\n",
    "    'nb_model': nb_model_fixed,\n",
    "    'nb_weight': float(nb_weight)\n",
    "}\n",
    "\n",
    "with open(\"models.json\", 'w') as file:\n",
    "    json.dump(save_obj, file, indent = 4)\n",
    "    print('Model saved to file models.json!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
